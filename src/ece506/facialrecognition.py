# -*- coding: utf-8 -*-
"""FacialRecognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QOOse3b3luST8iD7JzKWh8Il2C1_9Rlb
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
# %mkdir -p /content/drive/MyDrive/ECE_506_Project/ece506
# %cd /content/drive/MyDrive/ECE_506_Project

# !pip -q install kagglehub opencv-python
# !pip install transformers tensorflow scikit-learn

# Adjust Parameters to hardware e.g. GPU,TPU,CPU for best performance and to prevent crashing
# PARAMETERS
IMG_SIZE = 224
BATCH_SIZE = 64
EPOCHS_PER_FOLD = 45
N_SPLITS = 5
SAVE_DIR = "models"

# Standardize the classes between datasets
CK_plus_standard = ['Anger','Disgust','Fear','Happiness','Sadness','Surprise','Neutral']

CK_to_CK = {0:0, 1:1, 2:2, 3:3, 4:4, 5:5, 6:6}
RAF_to_CK = {1:5, 2:2, 3:1, 4:3, 5:4, 6:0, 7:6}
FER_to_CK = {'angry':0, 'disgust':1, 'fear':2, 'happy':3, 'sad':4, 'surprise':5, 'neutral':6}
AffectNet_to_CK = {0:0, 2:1, 3:2, 4:3, 6:4, 7:5, 5:6 }

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
import tensorflow as tf
from tensorflow.keras import layers, models, applications
from tensorflow.keras.optimizers import Adam
import cv2
import time
from tqdm import tqdm
import kagglehub
import random
import math
import tempfile
from tensorflow.keras.applications.efficientnet import preprocess_input
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.regularizers import l2

# from tensorflow.keras.applications import ResNet50

# Commented out IPython magic to ensure Python compatibility.
# %%writefile ece506/__init__.py

# Import RAF-DB dataset
print("Downloading / locating RAF-DB dataset...")
rafdb_path = kagglehub.dataset_download("shuvoalok/raf-db-dataset")
print("Path to RAF-DB dataset files:", rafdb_path)

# Import FER2013 dataset
print("Downloading / locating FER2013 dataset...")
fer2013_path = kagglehub.dataset_download("msambare/fer2013")
print("Path to FER2013 dataset files:", fer2013_path)

# Import CK+ dataset
print("Downloading / locating CK+ dataset...")
ckplus_path = kagglehub.dataset_download("davilsena/ckdataset")
print("Path to CK+ dataset files:", ckplus_path)

# Import AffectNet dataset
print("Downloading / locating AffectNet dataset...")
affectnet_path = kagglehub.dataset_download("fatihkgg/affectnet-yolo-format")
print("Path to AffectNet dataset files:", affectnet_path)

# %%writefile ece506/model.py
# from tensorflow.keras import Sequential, Model
# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense

# def build_cnn(input_shape, n_classes=7):
#     model = Sequential([
#         Input(shape=input_shape),
#         Conv2D(64, (3, 3), activation='relu'),
#         MaxPooling2D((2, 2)),
#         Conv2D(64, (3, 3), activation='relu'),
#         MaxPooling2D((2, 2)),
#         Conv2D(32, (3, 3), activation='relu'),
#         MaxPooling2D((2, 2)),
#         Flatten(),
#         Dense(128, activation='relu'),
#         Dense(n_classes, activation='softmax')
#     ])
#     model.compile(loss='sparse_categorical_crossentropy',
#                   optimizer='adam', metrics=['accuracy'])
#     return model

# def get_feature_extractor(model):
#     return Model(inputs=model.inputs, outputs=model.layers[-2].output)

import tensorflow as tf
import matplotlib.pyplot as plt
import os
from IPython.display import clear_output


PLOT_SAVE_DIR = '/content/drive/MyDrive/ECE_506_Project/training_plots'

class LivePlottingCallback(tf.keras.callbacks.Callback):
    def __init__(self, fold_number=None, dataset_name="", model_name="", update_frequency=5):
        super().__init__()
        self.fold_number = fold_number
        self.dataset_name = dataset_name
        self.model_name = model_name
        self.update_frequency = update_frequency
        self.epoch_history = []
        self.history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}
        self.save_dir = '/content/drive/MyDrive/ECE_506_Project/training_plots'
        os.makedirs(self.save_dir, exist_ok=True)


    def plot_and_show(self, epoch, is_final=False):
        clear_output(wait=True)

        title_suffix = f" - {self.dataset_name} - {self.model_name})"

        plt.figure(figsize=(12, 5))

        # Plot Loss
        plt.subplot(1, 2, 1)
        plt.plot(self.epoch_history, self.history['loss'], label='Training Loss', color='blue')
        plt.plot(self.epoch_history, self.history['val_loss'], label='Validation Loss', color='red')
        plt.title(f'Loss vs. Epochs{title_suffix}')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(axis='y', linestyle='--')

        # Plot Accuracy
        plt.subplot(1, 2, 2)
        plt.plot(self.epoch_history, self.history['accuracy'], label='Training Accuracy', color='blue')
        plt.plot(self.epoch_history, self.history['val_accuracy'], label='Validation Accuracy', color='red')
        plt.title(f'Accuracy vs. Epochs{title_suffix}')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(axis='y', linestyle='--')

        plt.tight_layout()

        if is_final:
            file_name = f'{self.dataset_name}_{self.model_name}_fold_{self.fold_number}_final.png'
            full_path = os.path.join(self.save_dir, file_name)
            plt.savefig(full_path)
            print(f"\nFinal plot saved to: {full_path}")

        plt.show()
        plt.close()

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}

        self.epoch_history.append(epoch + 1)
        for key in self.history.keys():
            self.history[key].append(logs.get(key))
        total_epochs = self.params['epochs']
        if (epoch + 1) % self.update_frequency == 0 or (epoch + 1) == total_epochs:
            self.plot_and_show(epoch, is_final=(epoch + 1) == total_epochs)

    def on_train_end(self, logs=None):
        clear_output(wait=True)

csv_file = os.path.join(ckplus_path, "ckextended.csv")
df = pd.read_csv(csv_file)

label_col = "emotion"
pixel_col = "pixels"


df = df[df[label_col] <= 6] # Keep only labels 0-6

def pixels_to_array(pixel_str, img_size=IMG_SIZE):
    pixels = np.array([int(p) for p in pixel_str.split()], dtype="float32")
    pixels = pixels.reshape((48,48))
    pixels = cv2.resize(pixels, (img_size,img_size))
    return pixels

X_ckplus = np.array([pixels_to_array(p) for p in df[pixel_col]], dtype="float32")
y_ckplus = df[label_col].to_numpy(dtype="int64")

# Temporary folder for CK+ images
TEMP_CK_DIR = tempfile.mkdtemp(prefix="ckplus_train_")
for label in range(7):
    os.makedirs(os.path.join(TEMP_CK_DIR, str(label)), exist_ok=True)

for idx, (img_array, label) in enumerate(tqdm(zip(X_ckplus, y_ckplus), total=len(y_ckplus))):
    img_uint8 = (img_array * 255).astype(np.uint8)
    # Convert grayscale to RGB
    img_rgb = cv2.merge([img_uint8, img_uint8, img_uint8])
    img_path = os.path.join(TEMP_CK_DIR, str(label), f"{idx}.png")
    cv2.imwrite(img_path, img_rgb)

print("CK+ dataset converted to RGB at:", TEMP_CK_DIR)

def dataset_batch_generator(paths_labels, batch_size=BATCH_SIZE, img_size=IMG_SIZE, shuffle=True):

    n = len(paths_labels)
    while True:
        if shuffle:
            random.shuffle(paths_labels)
        for start in range(0, n, batch_size):
            end = start + batch_size
            batch = paths_labels[start:end]
            X_batch, Y_batch = [], []
            for img_path, label in batch:
                img = cv2.imread(img_path)
                if img is None:
                    continue
                img = cv2.resize(img, (img_size, img_size))
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)# Convert to RGB
                img = img.astype("float32")
                img = preprocess_input(img)

                X_batch.append(img)
                Y_batch.append(int(label))

            if len(X_batch) == 0:
                continue

            yield np.array(X_batch), np.array(Y_batch)

def print_batch_sizes(dataset_dir, label_map=None, dataset_name="Dataset"):
    classes = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]
    classes.sort()
    paths_labels = []
    for idx, cls in enumerate(classes):
        folder = os.path.join(dataset_dir, cls)
        for img_file in os.listdir(folder):
            img_path = os.path.join(folder, img_file)
            if label_map is not None:
                if isinstance(cls, str) and cls in label_map:
                    label = label_map[cls]
                else:
                    label = label_map.get(idx, idx)
            else:
                label = idx
            paths_labels.append((img_path, label))

    gen = dataset_batch_generator(paths_labels, batch_size=BATCH_SIZE)
    X_batch, Y_batch = next(gen)
    print(f"{dataset_name} - First batch shapes:")
    print("X_batch:", X_batch.shape)
    print("Y_batch:", Y_batch.shape)
    print("-" * 40)

# Print batch sizes
raf_train_dir = os.path.join(rafdb_path, "DATASET/train")
fer_train_dir = os.path.join(fer2013_path, "train")

print_batch_sizes(raf_train_dir, label_map=RAF_to_CK, dataset_name="RAF-DB")
print_batch_sizes(fer_train_dir, label_map=FER_to_CK, dataset_name="FER2013")
print_batch_sizes(TEMP_CK_DIR, label_map=CK_to_CK, dataset_name="CK+")

def get_data_augmentation_layers(img_size):
    return tf.keras.Sequential([
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.1)
    ], name="data_augmentation")

class Model_Trainer:
    def __init__(self, model_name, dataset_dir=None, label_map=None, n_classes=7,
                 batch_size=BATCH_SIZE, input_shape=(IMG_SIZE, IMG_SIZE, 3),
                 epochs_per_fold=EPOCHS_PER_FOLD, n_splits=N_SPLITS, save_dir=SAVE_DIR):
        self.model_name = model_name
        self.dataset_dir = dataset_dir
        self.label_map = label_map
        self.n_classes = n_classes
        self.batch_size = batch_size
        self.input_shape = input_shape
        self.epochs_per_fold = epochs_per_fold
        self.n_splits = n_splits
        self.save_dir = save_dir

        os.makedirs(save_dir, exist_ok=True)
        self.global_best_model_path = None
        self.global_best_acc = 0
        self.best_hyperparameters = {}

    def build_model(self, dense_size, dropout_rate, l2_rate, learning_rate, freeze_base):

        inputs = layers.Input(shape=self.input_shape)
        x = get_data_augmentation_layers(self.input_shape[0])(inputs)

        try:
            base_model_cls = getattr(applications, self.model_name)
            base_model = base_model_cls(
                include_top=False,
                weights='imagenet',
                input_tensor=x,
                pooling='avg'
            )

        except Exception as e:
            print(f"ERROR: Could not load model {self.model_name}. Keras Application not found. Error: {e}")
            return None


        base_model.trainable = not freeze_base

        x = base_model.output

        x = layers.Dense(dense_size, activation='relu', kernel_regularizer=l2(l2_rate))(x)
        x = layers.Dropout(dropout_rate)(x)
        out = layers.Dense(self.n_classes, activation='softmax')(x)

        model = models.Model(inputs=inputs, outputs=out)
        model.compile(optimizer=Adam(learning_rate=learning_rate),
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])
        return model

    def train(self):
        if self.dataset_dir is None:
            raise ValueError("dataset_dir must be provided to train the model.")
        return self.train_vfold(self.dataset_dir, self.label_map)

    def train_vfold(self, dataset_dir, label_map=None):
        classes = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]
        classes.sort()

        paths_labels = []
        for cls_name in classes:
            folder = os.path.join(dataset_dir, cls_name)

            label = None
            if label_map is not None:
                label = label_map.get(cls_name)

                if label is None and cls_name.isdigit():
                    try:
                        label = label_map.get(int(cls_name))
                    except ValueError:
                        pass
            elif cls_name.isdigit():
                try:
                    label = int(cls_name)
                except ValueError:
                    pass

            if label is None or not (0 <= label < self.n_classes):
                 continue

            for img_file in os.listdir(folder):
                img_path = os.path.join(folder, img_file)
                paths_labels.append((img_path, label))

        if not paths_labels:
            raise ValueError(f"No image samples found in directory: {dataset_dir}. Check folder names or label map.")

        labels = [label for _, label in paths_labels]
        N_SAMPLES = len(labels)

        # --- HYPERPARAMETER SELECTION ---
        if N_SAMPLES <= 2000:
            DENSE_SIZE = 128
            DROPOUT_RATE = 0.7
            L2_RATE = 0.01
            LR_RATE = 5e-5
            FREEZE_BASE = True
            print(f"Dataset Size: SMALL ({N_SAMPLES} samples). Applying aggressive regularization (128 units, Drop 0.7, L2 0.01, LR 5e-5) and freezing base.")
        else:
            DENSE_SIZE = 256
            DROPOUT_RATE = 0.7
            L2_RATE = 0.01
            LR_RATE = 2e-5
            FREEZE_BASE = False
            print(f"Dataset Size: LARGE ({N_SAMPLES} samples). Applying moderate regularization (384 units, Drop 0.6, L2 0.005, LR 7e-5) and unfreezing base.")

        # --- K-FOLD CROSS-VALIDATION LOOP ---
        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)

        for fold_num, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels), 1):
            print(f"\n=== Fold {fold_num}/{self.n_splits} ===")
            train_subset = [paths_labels[i] for i in train_idx]
            val_subset = [paths_labels[i] for i in val_idx]

            train_gen = dataset_batch_generator(train_subset, batch_size=self.batch_size, shuffle=True, img_size=self.input_shape[0])
            val_gen = dataset_batch_generator(val_subset, batch_size=self.batch_size, shuffle=False, img_size=self.input_shape[0])

            steps_per_epoch = math.ceil(len(train_subset)/self.batch_size)
            validation_steps = math.ceil(len(val_subset)/self.batch_size)

            model = self.build_model(
                dense_size=DENSE_SIZE,
                dropout_rate=DROPOUT_RATE,
                l2_rate=L2_RATE,
                learning_rate=LR_RATE,
                freeze_base=FREEZE_BASE
            )

            if model is None:
                print(f"Skipping fold {fold_num} due to model loading error.")
                continue

            # FILE NAMING
            model_file_name = self.model_name.replace('/', '_').replace('-', '_')
            dataset_part = dataset_dir.split('/')[-1]

            # Example: ResNet50_raf_train_fold_1
            fold_dir_name = f"{model_file_name}_{dataset_part}_fold_{fold_num}"
            fold_dir = os.path.join(self.save_dir, fold_dir_name)
            os.makedirs(fold_dir, exist_ok=True)

            dynamic_filename = f"{model_file_name}_{dataset_part}.h5"
            final_model_path = os.path.join(fold_dir, dynamic_filename)


            early_stopping_cb = EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True,
                verbose=1
            )

            model_checkpoint_cb = ModelCheckpoint(
                filepath=final_model_path,
                monitor='val_loss',
                save_best_only=True,
                save_weights_only=False,
                verbose=0
            )

            # Comment out live_plotting_cb for training Summary
            live_plotting_cb = LivePlottingCallback(
                fold_number=fold_num,
                dataset_name=dataset_name,
                model_name=model_name,
                update_frequency=2
            )

            callbacks = [
                live_plotting_cb,
                early_stopping_cb,
                model_checkpoint_cb
            ]

            history = model.fit(
                train_gen,
                steps_per_epoch=steps_per_epoch,
                validation_data=val_gen,
                validation_steps=validation_steps,
                epochs=self.epochs_per_fold,
                callbacks=callbacks,
                verbose=1
            )


            # Evaluate the restored best weights
            loss, best_val_acc = model.evaluate(val_gen, steps=validation_steps, verbose=0)

            print(f"Best validation accuracy for fold {fold_num}: {best_val_acc:.4f}")

            if best_val_acc > self.global_best_acc:
                self.global_best_acc = best_val_acc
                self.global_best_model_path = final_model_path

                self.best_hyperparameters = {
                    'dataset_samples': N_SAMPLES,
                    'dense_size': DENSE_SIZE,
                    'dropout_rate': DROPOUT_RATE,
                    'l2_rate': L2_RATE,
                    'learning_rate': LR_RATE,
                    'freeze_base': FREEZE_BASE,
                    'best_fold': fold_num
                }

        print("\n=== V-Fold Training Finished ===")
        print(f"Global best model: {self.global_best_model_path}")
        print(f"Global best validation accuracy: {self.global_best_acc:.4f}")
        print(f"Best Hyperparameters: {self.best_hyperparameters}")
        return self.global_best_model_path

# --- 1. Define Model Names ---
MODEL_NAMES = [
    "ResNet50",
    "VGG16",
    "InceptionV3",
    "MobileNetV3Small",
    "EfficientNetB0",
]

DATASETS = [
    {
        "name": "FER2013",
        "dir": fer_train_dir,
        "map": FER_to_CK,
    },
    {
        "name": "CK+",
        "dir": TEMP_CK_DIR,
        "map": CK_to_CK,
    },
    {
        "name": "RAF-DB",
        "dir": raf_train_dir,
        "map": RAF_to_CK,
    },
]

# Dictionary to store the results
all_results = []
os.makedirs(SAVE_DIR, exist_ok=True)


# --- 3. Outer Loop: Iterate over Datasets ---
for dataset_info in DATASETS:
    dataset_name = dataset_info["name"]
    dataset_dir = dataset_info["dir"]
    label_map = dataset_info["map"]

    # --- Inner Loop: Iterate over Models ---
    for model_name in MODEL_NAMES:

        # Create a unique directory for this combination to save the best model
        safe_model_name = model_name.replace('-', '_').replace('/', '_').replace(' ', '_')
        combo_save_dir = os.path.join(SAVE_DIR, f"{dataset_name}_{safe_model_name}")

        print(f" Starting Training: {model_name} on {dataset_name} (Saving to: {combo_save_dir})")

        try:
            trainer = Model_Trainer(
                model_name=model_name,
                dataset_dir=dataset_dir,
                label_map=label_map,
                input_shape=(IMG_SIZE, IMG_SIZE, 3),
                batch_size=BATCH_SIZE,
                epochs_per_fold=EPOCHS_PER_FOLD,
                save_dir=combo_save_dir
            )

            best_model_path = trainer.train()

            # Save the final result
            all_results.append({
                "dataset": dataset_name,
                "model": model_name,
                "best_acc": trainer.global_best_acc,
                "best_path": best_model_path
            })

        except Exception as e:
            print(f" ERROR training {model_name} on {dataset_name}: {e}")
            all_results.append({
                "dataset": dataset_name,
                "model": model_name,
                "best_acc": 0.0,
                "best_path": "ERROR"
            })


print(" ALL TRAINING COMPLETE!!")


final_summary_df = pd.DataFrame(all_results)
print(final_summary_df)

summary_file = os.path.join(SAVE_DIR, "all_models_training_summary.csv")
final_summary_df.to_csv(summary_file, index=False)
print(f"\nFull training summary saved to: {summary_file}")

# # 1. Define the custom object function (must be present in your new script)
# def get_data_augmentation_layers(img_size):
#     return tf.keras.Sequential([
#         tf.keras.layers.RandomFlip("horizontal"),
#         tf.keras.layers.RandomRotation(0.1),
#         tf.keras.layers.RandomZoom(0.1)
#     ], name="data_augmentation")

# # 2. Specify the path to your best saved model
# BEST_MODEL_PATH = "best_model.h5"

# # 3. Load the model
# loaded_model = tf.keras.models.load_model(
#     BEST_MODEL_PATH,
#     custom_objects={'get_data_augmentation_layers': get_data_augmentation_layers}
# )

# print(f"Model {os.path.basename(BEST_MODEL_PATH)} loaded successfully.")